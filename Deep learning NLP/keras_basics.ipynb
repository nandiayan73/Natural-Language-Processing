{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "print(iris.data)\n",
    "x=iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "y=iris.target\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since there are three data in the output, we have to categorize by one hunt coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y=to_categorical(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.7 2.9 4.2 1.3]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.7 3.  5.  1.7]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.  2.2 5.  1.5]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [7.1 3.  5.9 2.1]]\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.33,random_state=42)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing the data:\n",
    "Standardization just means normalizing the values to all fit between a certain range, like 0-1, or -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_object=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.7 2.9 4.2 1.3]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.7 3.  5.  1.7]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.  2.2 5.  1.5]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [7.1 3.  5.9 2.1]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train)\n",
    "scaler_object.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41176471 0.40909091 0.55357143 0.5       ]\n",
      " [0.97058824 0.45454545 0.98214286 0.83333333]\n",
      " [0.38235294 0.45454545 0.60714286 0.58333333]\n",
      " [0.23529412 0.68181818 0.05357143 0.04166667]\n",
      " [1.         0.36363636 1.         0.79166667]\n",
      " [0.44117647 0.31818182 0.53571429 0.375     ]\n",
      " [0.26470588 0.63636364 0.05357143 0.04166667]\n",
      " [0.20588235 0.68181818 0.03571429 0.08333333]\n",
      " [0.23529412 0.81818182 0.14285714 0.125     ]\n",
      " [0.20588235 0.         0.42857143 0.375     ]\n",
      " [0.58823529 0.31818182 0.67857143 0.70833333]\n",
      " [0.14705882 0.63636364 0.14285714 0.04166667]\n",
      " [0.20588235 0.45454545 0.08928571 0.04166667]\n",
      " [0.23529412 0.59090909 0.10714286 0.16666667]\n",
      " [0.38235294 0.31818182 0.55357143 0.5       ]\n",
      " [0.23529412 0.63636364 0.07142857 0.04166667]\n",
      " [0.41176471 0.45454545 0.55357143 0.45833333]\n",
      " [1.         0.81818182 1.         0.875     ]\n",
      " [0.08823529 0.54545455 0.05357143 0.04166667]\n",
      " [0.55882353 0.40909091 0.57142857 0.5       ]\n",
      " [0.41176471 0.22727273 0.69642857 0.79166667]\n",
      " [0.35294118 1.         0.05357143 0.04166667]\n",
      " [0.5        0.45454545 0.66071429 0.70833333]\n",
      " [0.44117647 0.31818182 0.71428571 0.75      ]\n",
      " [0.5        0.09090909 0.51785714 0.375     ]\n",
      " [0.32352941 0.45454545 0.60714286 0.58333333]\n",
      " [0.55882353 0.63636364 0.76785714 0.91666667]\n",
      " [0.35294118 0.13636364 0.51785714 0.5       ]\n",
      " [0.32352941 0.86363636 0.10714286 0.125     ]\n",
      " [0.20588235 0.13636364 0.39285714 0.375     ]\n",
      " [0.61764706 0.31818182 0.75       0.75      ]\n",
      " [0.20588235 0.59090909 0.05357143 0.04166667]\n",
      " [0.20588235 0.54545455 0.01785714 0.04166667]\n",
      " [0.35294118 0.18181818 0.48214286 0.41666667]\n",
      " [0.70588235 0.45454545 0.69642857 0.66666667]\n",
      " [0.17647059 0.5        0.07142857 0.04166667]\n",
      " [0.44117647 0.36363636 0.71428571 0.95833333]\n",
      " [0.20588235 0.63636364 0.07142857 0.04166667]\n",
      " [0.20588235 0.68181818 0.08928571 0.20833333]\n",
      " [0.47058824 0.54545455 0.66071429 0.70833333]\n",
      " [0.23529412 0.22727273 0.33928571 0.41666667]\n",
      " [0.76470588 0.54545455 0.82142857 0.91666667]\n",
      " [0.5        0.31818182 0.71428571 0.625     ]\n",
      " [0.52941176 0.27272727 0.80357143 0.54166667]\n",
      " [1.         0.45454545 0.89285714 0.91666667]\n",
      " [0.35294118 0.22727273 0.51785714 0.5       ]\n",
      " [0.02941176 0.40909091 0.05357143 0.04166667]\n",
      " [0.         0.45454545 0.         0.        ]\n",
      " [0.5        0.09090909 0.69642857 0.58333333]\n",
      " [0.85294118 0.54545455 0.875      0.70833333]\n",
      " [0.08823529 0.5        0.07142857 0.04166667]\n",
      " [0.23529412 0.68181818 0.05357143 0.08333333]\n",
      " [0.02941176 0.45454545 0.03571429 0.04166667]\n",
      " [0.58823529 0.22727273 0.67857143 0.58333333]\n",
      " [0.58823529 0.63636364 0.80357143 0.95833333]\n",
      " [0.08823529 0.63636364 0.05357143 0.08333333]\n",
      " [0.73529412 0.45454545 0.78571429 0.83333333]\n",
      " [0.58823529 0.59090909 0.875      1.        ]\n",
      " [0.11764706 0.54545455 0.03571429 0.04166667]\n",
      " [0.52941176 0.40909091 0.64285714 0.54166667]\n",
      " [0.64705882 0.36363636 0.625      0.58333333]\n",
      " [0.55882353 0.36363636 0.66071429 0.70833333]\n",
      " [0.79411765 0.54545455 0.64285714 0.54166667]\n",
      " [0.61764706 0.54545455 0.75       0.91666667]\n",
      " [0.23529412 0.81818182 0.08928571 0.04166667]\n",
      " [0.76470588 0.5        0.76785714 0.83333333]\n",
      " [0.47058824 0.45454545 0.55357143 0.58333333]\n",
      " [0.64705882 0.45454545 0.73214286 0.79166667]\n",
      " [0.41176471 0.27272727 0.42857143 0.375     ]\n",
      " [0.26470588 0.31818182 0.5        0.54166667]\n",
      " [0.52941176 0.45454545 0.625      0.54166667]\n",
      " [0.05882353 0.13636364 0.03571429 0.08333333]\n",
      " [0.67647059 0.40909091 0.625      0.5       ]\n",
      " [0.35294118 0.27272727 0.58928571 0.45833333]\n",
      " [0.29411765 0.77272727 0.07142857 0.04166667]\n",
      " [0.38235294 0.45454545 0.53571429 0.5       ]\n",
      " [0.88235294 0.40909091 0.92857143 0.70833333]\n",
      " [0.70588235 0.59090909 0.82142857 0.83333333]\n",
      " [0.23529412 0.77272727 0.07142857 0.125     ]\n",
      " [0.17647059 0.18181818 0.39285714 0.375     ]\n",
      " [0.70588235 0.59090909 0.82142857 1.        ]\n",
      " [0.85294118 0.45454545 0.83928571 0.625     ]\n",
      " [0.17647059 0.72727273 0.05357143 0.        ]\n",
      " [0.70588235 0.5        0.80357143 0.95833333]\n",
      " [0.17647059 0.45454545 0.05357143 0.04166667]\n",
      " [0.76470588 0.5        0.67857143 0.58333333]\n",
      " [0.91176471 0.36363636 0.89285714 0.75      ]\n",
      " [0.58823529 0.40909091 0.80357143 0.70833333]\n",
      " [0.41176471 0.36363636 0.53571429 0.5       ]\n",
      " [0.64705882 0.45454545 0.78571429 0.70833333]\n",
      " [0.58823529 0.13636364 0.58928571 0.5       ]\n",
      " [0.61764706 0.40909091 0.57142857 0.5       ]\n",
      " [0.38235294 0.36363636 0.67857143 0.79166667]\n",
      " [0.47058824 0.45454545 0.71428571 0.70833333]\n",
      " [0.32352941 0.63636364 0.10714286 0.04166667]\n",
      " [0.52941176 0.36363636 0.51785714 0.5       ]\n",
      " [0.17647059 0.22727273 0.60714286 0.66666667]\n",
      " [0.44117647 0.90909091 0.01785714 0.04166667]\n",
      " [0.44117647 0.27272727 0.51785714 0.45833333]\n",
      " [0.82352941 0.45454545 0.85714286 0.83333333]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaled_x_train=scaler_object.transform(x_train)\n",
    "print(scaled_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.52941176  0.36363636  0.64285714  0.45833333]\n",
      " [ 0.41176471  0.81818182  0.10714286  0.08333333]\n",
      " [ 1.          0.27272727  1.03571429  0.91666667]\n",
      " [ 0.5         0.40909091  0.60714286  0.58333333]\n",
      " [ 0.73529412  0.36363636  0.66071429  0.54166667]\n",
      " [ 0.32352941  0.63636364  0.07142857  0.125     ]\n",
      " [ 0.38235294  0.40909091  0.44642857  0.5       ]\n",
      " [ 0.76470588  0.5         0.71428571  0.91666667]\n",
      " [ 0.55882353  0.09090909  0.60714286  0.58333333]\n",
      " [ 0.44117647  0.31818182  0.5         0.45833333]\n",
      " [ 0.64705882  0.54545455  0.71428571  0.79166667]\n",
      " [ 0.14705882  0.45454545  0.05357143  0.        ]\n",
      " [ 0.35294118  0.68181818  0.03571429  0.04166667]\n",
      " [ 0.17647059  0.5         0.07142857  0.        ]\n",
      " [ 0.23529412  0.81818182  0.07142857  0.08333333]\n",
      " [ 0.58823529  0.59090909  0.64285714  0.625     ]\n",
      " [ 0.64705882  0.45454545  0.83928571  0.875     ]\n",
      " [ 0.38235294  0.22727273  0.5         0.41666667]\n",
      " [ 0.41176471  0.36363636  0.60714286  0.5       ]\n",
      " [ 0.61764706  0.36363636  0.80357143  0.875     ]\n",
      " [ 0.11764706  0.54545455  0.08928571  0.04166667]\n",
      " [ 0.52941176  0.45454545  0.67857143  0.70833333]\n",
      " [ 0.20588235  0.63636364  0.08928571  0.125     ]\n",
      " [ 0.61764706  0.36363636  0.80357143  0.83333333]\n",
      " [ 1.05882353  0.81818182  0.94642857  0.79166667]\n",
      " [ 0.70588235  0.45454545  0.73214286  0.91666667]\n",
      " [ 0.70588235  0.22727273  0.83928571  0.70833333]\n",
      " [ 0.73529412  0.54545455  0.85714286  0.91666667]\n",
      " [ 0.14705882  0.45454545  0.05357143  0.08333333]\n",
      " [ 0.14705882  0.5         0.08928571  0.04166667]\n",
      " [ 0.08823529  0.72727273 -0.01785714  0.04166667]\n",
      " [ 0.41176471  1.09090909  0.07142857  0.125     ]\n",
      " [ 0.70588235  0.5         0.58928571  0.54166667]\n",
      " [ 0.14705882  0.63636364  0.08928571  0.04166667]\n",
      " [ 0.02941176  0.54545455  0.03571429  0.04166667]\n",
      " [ 0.58823529  0.22727273  0.69642857  0.75      ]\n",
      " [ 0.61764706  0.54545455  0.60714286  0.58333333]\n",
      " [ 0.26470588  0.68181818  0.07142857  0.04166667]\n",
      " [ 0.20588235  0.72727273  0.05357143  0.04166667]\n",
      " [ 0.26470588  0.95454545  0.07142857  0.        ]\n",
      " [ 0.44117647  0.31818182  0.71428571  0.75      ]\n",
      " [ 0.5         0.63636364  0.60714286  0.625     ]\n",
      " [ 0.70588235  0.5         0.64285714  0.58333333]\n",
      " [ 0.32352941  0.86363636  0.03571429  0.125     ]\n",
      " [ 0.32352941  0.77272727  0.07142857  0.04166667]\n",
      " [ 0.35294118  0.18181818  0.46428571  0.375     ]\n",
      " [ 0.58823529  0.36363636  0.71428571  0.58333333]\n",
      " [ 0.61764706  0.5         0.78571429  0.70833333]\n",
      " [ 0.67647059  0.45454545  0.58928571  0.54166667]\n",
      " [ 0.85294118  0.72727273  0.89285714  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "scaled_x_test=scaler_object.transform(x_test)\n",
    "print(scaled_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_x_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network with Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(8,input_dim=4,activation='relu'))#Input Layer\n",
    "model.add(Dense(8,input_dim=4,activation='relu'))#Hidden layer\n",
    "model.add(Dense(3,activation='softmax'))#Output Layer\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 8)                 40        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3)                 27        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 139 (556.00 Byte)\n",
      "Trainable params: 139 (556.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 1s - loss: 1.1223 - accuracy: 0.3100 - 763ms/epoch - 191ms/step\n",
      "Epoch 2/150\n",
      "4/4 - 0s - loss: 1.1148 - accuracy: 0.3200 - 15ms/epoch - 4ms/step\n",
      "Epoch 3/150\n",
      "4/4 - 0s - loss: 1.1080 - accuracy: 0.3200 - 12ms/epoch - 3ms/step\n",
      "Epoch 4/150\n",
      "4/4 - 0s - loss: 1.1017 - accuracy: 0.3500 - 12ms/epoch - 3ms/step\n",
      "Epoch 5/150\n",
      "4/4 - 0s - loss: 1.0960 - accuracy: 0.3600 - 13ms/epoch - 3ms/step\n",
      "Epoch 6/150\n",
      "4/4 - 0s - loss: 1.0902 - accuracy: 0.3700 - 10ms/epoch - 3ms/step\n",
      "Epoch 7/150\n",
      "4/4 - 0s - loss: 1.0845 - accuracy: 0.3700 - 11ms/epoch - 3ms/step\n",
      "Epoch 8/150\n",
      "4/4 - 0s - loss: 1.0786 - accuracy: 0.3800 - 9ms/epoch - 2ms/step\n",
      "Epoch 9/150\n",
      "4/4 - 0s - loss: 1.0725 - accuracy: 0.4100 - 11ms/epoch - 3ms/step\n",
      "Epoch 10/150\n",
      "4/4 - 0s - loss: 1.0664 - accuracy: 0.4400 - 10ms/epoch - 3ms/step\n",
      "Epoch 11/150\n",
      "4/4 - 0s - loss: 1.0602 - accuracy: 0.4700 - 8ms/epoch - 2ms/step\n",
      "Epoch 12/150\n",
      "4/4 - 0s - loss: 1.0539 - accuracy: 0.5300 - 7ms/epoch - 2ms/step\n",
      "Epoch 13/150\n",
      "4/4 - 0s - loss: 1.0476 - accuracy: 0.5700 - 10ms/epoch - 3ms/step\n",
      "Epoch 14/150\n",
      "4/4 - 0s - loss: 1.0410 - accuracy: 0.6200 - 8ms/epoch - 2ms/step\n",
      "Epoch 15/150\n",
      "4/4 - 0s - loss: 1.0343 - accuracy: 0.6400 - 8ms/epoch - 2ms/step\n",
      "Epoch 16/150\n",
      "4/4 - 0s - loss: 1.0279 - accuracy: 0.6400 - 6ms/epoch - 1ms/step\n",
      "Epoch 17/150\n",
      "4/4 - 0s - loss: 1.0216 - accuracy: 0.6400 - 7ms/epoch - 2ms/step\n",
      "Epoch 18/150\n",
      "4/4 - 0s - loss: 1.0151 - accuracy: 0.6400 - 6ms/epoch - 2ms/step\n",
      "Epoch 19/150\n",
      "4/4 - 0s - loss: 1.0087 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 20/150\n",
      "4/4 - 0s - loss: 1.0023 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 21/150\n",
      "4/4 - 0s - loss: 0.9960 - accuracy: 0.6600 - 8ms/epoch - 2ms/step\n",
      "Epoch 22/150\n",
      "4/4 - 0s - loss: 0.9891 - accuracy: 0.6800 - 8ms/epoch - 2ms/step\n",
      "Epoch 23/150\n",
      "4/4 - 0s - loss: 0.9817 - accuracy: 0.6800 - 7ms/epoch - 2ms/step\n",
      "Epoch 24/150\n",
      "4/4 - 0s - loss: 0.9740 - accuracy: 0.6800 - 10ms/epoch - 2ms/step\n",
      "Epoch 25/150\n",
      "4/4 - 0s - loss: 0.9666 - accuracy: 0.6600 - 6ms/epoch - 2ms/step\n",
      "Epoch 26/150\n",
      "4/4 - 0s - loss: 0.9588 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 27/150\n",
      "4/4 - 0s - loss: 0.9513 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 28/150\n",
      "4/4 - 0s - loss: 0.9436 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 29/150\n",
      "4/4 - 0s - loss: 0.9357 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 30/150\n",
      "4/4 - 0s - loss: 0.9278 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 31/150\n",
      "4/4 - 0s - loss: 0.9196 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 32/150\n",
      "4/4 - 0s - loss: 0.9114 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 33/150\n",
      "4/4 - 0s - loss: 0.9030 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 34/150\n",
      "4/4 - 0s - loss: 0.8944 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 35/150\n",
      "4/4 - 0s - loss: 0.8857 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 36/150\n",
      "4/4 - 0s - loss: 0.8770 - accuracy: 0.6500 - 9ms/epoch - 2ms/step\n",
      "Epoch 37/150\n",
      "4/4 - 0s - loss: 0.8681 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 38/150\n",
      "4/4 - 0s - loss: 0.8588 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 39/150\n",
      "4/4 - 0s - loss: 0.8495 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 40/150\n",
      "4/4 - 0s - loss: 0.8408 - accuracy: 0.6500 - 6ms/epoch - 1ms/step\n",
      "Epoch 41/150\n",
      "4/4 - 0s - loss: 0.8312 - accuracy: 0.6500 - 10ms/epoch - 3ms/step\n",
      "Epoch 42/150\n",
      "4/4 - 0s - loss: 0.8219 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 43/150\n",
      "4/4 - 0s - loss: 0.8127 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 44/150\n",
      "4/4 - 0s - loss: 0.8033 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 45/150\n",
      "4/4 - 0s - loss: 0.7940 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 46/150\n",
      "4/4 - 0s - loss: 0.7852 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 47/150\n",
      "4/4 - 0s - loss: 0.7755 - accuracy: 0.6500 - 9ms/epoch - 2ms/step\n",
      "Epoch 48/150\n",
      "4/4 - 0s - loss: 0.7661 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 49/150\n",
      "4/4 - 0s - loss: 0.7570 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 50/150\n",
      "4/4 - 0s - loss: 0.7480 - accuracy: 0.6500 - 6ms/epoch - 2ms/step\n",
      "Epoch 51/150\n",
      "4/4 - 0s - loss: 0.7392 - accuracy: 0.6500 - 8ms/epoch - 2ms/step\n",
      "Epoch 52/150\n",
      "4/4 - 0s - loss: 0.7306 - accuracy: 0.6500 - 7ms/epoch - 2ms/step\n",
      "Epoch 53/150\n",
      "4/4 - 0s - loss: 0.7215 - accuracy: 0.6600 - 7ms/epoch - 2ms/step\n",
      "Epoch 54/150\n",
      "4/4 - 0s - loss: 0.7130 - accuracy: 0.6700 - 6ms/epoch - 2ms/step\n",
      "Epoch 55/150\n",
      "4/4 - 0s - loss: 0.7051 - accuracy: 0.6700 - 7ms/epoch - 2ms/step\n",
      "Epoch 56/150\n",
      "4/4 - 0s - loss: 0.6965 - accuracy: 0.6800 - 8ms/epoch - 2ms/step\n",
      "Epoch 57/150\n",
      "4/4 - 0s - loss: 0.6884 - accuracy: 0.6800 - 7ms/epoch - 2ms/step\n",
      "Epoch 58/150\n",
      "4/4 - 0s - loss: 0.6804 - accuracy: 0.6800 - 8ms/epoch - 2ms/step\n",
      "Epoch 59/150\n",
      "4/4 - 0s - loss: 0.6724 - accuracy: 0.6800 - 8ms/epoch - 2ms/step\n",
      "Epoch 60/150\n",
      "4/4 - 0s - loss: 0.6648 - accuracy: 0.6800 - 7ms/epoch - 2ms/step\n",
      "Epoch 61/150\n",
      "4/4 - 0s - loss: 0.6576 - accuracy: 0.6800 - 7ms/epoch - 2ms/step\n",
      "Epoch 62/150\n",
      "4/4 - 0s - loss: 0.6502 - accuracy: 0.6900 - 5ms/epoch - 1ms/step\n",
      "Epoch 63/150\n",
      "4/4 - 0s - loss: 0.6435 - accuracy: 0.7100 - 6ms/epoch - 2ms/step\n",
      "Epoch 64/150\n",
      "4/4 - 0s - loss: 0.6370 - accuracy: 0.7200 - 7ms/epoch - 2ms/step\n",
      "Epoch 65/150\n",
      "4/4 - 0s - loss: 0.6306 - accuracy: 0.7500 - 8ms/epoch - 2ms/step\n",
      "Epoch 66/150\n",
      "4/4 - 0s - loss: 0.6243 - accuracy: 0.7600 - 6ms/epoch - 2ms/step\n",
      "Epoch 67/150\n",
      "4/4 - 0s - loss: 0.6184 - accuracy: 0.7400 - 8ms/epoch - 2ms/step\n",
      "Epoch 68/150\n",
      "4/4 - 0s - loss: 0.6119 - accuracy: 0.7400 - 7ms/epoch - 2ms/step\n",
      "Epoch 69/150\n",
      "4/4 - 0s - loss: 0.6066 - accuracy: 0.7800 - 8ms/epoch - 2ms/step\n",
      "Epoch 70/150\n",
      "4/4 - 0s - loss: 0.6011 - accuracy: 0.7800 - 7ms/epoch - 2ms/step\n",
      "Epoch 71/150\n",
      "4/4 - 0s - loss: 0.5951 - accuracy: 0.7800 - 8ms/epoch - 2ms/step\n",
      "Epoch 72/150\n",
      "4/4 - 0s - loss: 0.5895 - accuracy: 0.7800 - 8ms/epoch - 2ms/step\n",
      "Epoch 73/150\n",
      "4/4 - 0s - loss: 0.5842 - accuracy: 0.7700 - 8ms/epoch - 2ms/step\n",
      "Epoch 74/150\n",
      "4/4 - 0s - loss: 0.5788 - accuracy: 0.7700 - 8ms/epoch - 2ms/step\n",
      "Epoch 75/150\n",
      "4/4 - 0s - loss: 0.5738 - accuracy: 0.7700 - 8ms/epoch - 2ms/step\n",
      "Epoch 76/150\n",
      "4/4 - 0s - loss: 0.5689 - accuracy: 0.7700 - 6ms/epoch - 1ms/step\n",
      "Epoch 77/150\n",
      "4/4 - 0s - loss: 0.5644 - accuracy: 0.7700 - 7ms/epoch - 2ms/step\n",
      "Epoch 78/150\n",
      "4/4 - 0s - loss: 0.5600 - accuracy: 0.7700 - 7ms/epoch - 2ms/step\n",
      "Epoch 79/150\n",
      "4/4 - 0s - loss: 0.5554 - accuracy: 0.7600 - 9ms/epoch - 2ms/step\n",
      "Epoch 80/150\n",
      "4/4 - 0s - loss: 0.5508 - accuracy: 0.7700 - 8ms/epoch - 2ms/step\n",
      "Epoch 81/150\n",
      "4/4 - 0s - loss: 0.5465 - accuracy: 0.7700 - 8ms/epoch - 2ms/step\n",
      "Epoch 82/150\n",
      "4/4 - 0s - loss: 0.5421 - accuracy: 0.7700 - 11ms/epoch - 3ms/step\n",
      "Epoch 83/150\n",
      "4/4 - 0s - loss: 0.5379 - accuracy: 0.7700 - 10ms/epoch - 2ms/step\n",
      "Epoch 84/150\n",
      "4/4 - 0s - loss: 0.5337 - accuracy: 0.7900 - 8ms/epoch - 2ms/step\n",
      "Epoch 85/150\n",
      "4/4 - 0s - loss: 0.5298 - accuracy: 0.8000 - 9ms/epoch - 2ms/step\n",
      "Epoch 86/150\n",
      "4/4 - 0s - loss: 0.5262 - accuracy: 0.8100 - 10ms/epoch - 3ms/step\n",
      "Epoch 87/150\n",
      "4/4 - 0s - loss: 0.5222 - accuracy: 0.8200 - 7ms/epoch - 2ms/step\n",
      "Epoch 88/150\n",
      "4/4 - 0s - loss: 0.5184 - accuracy: 0.8100 - 8ms/epoch - 2ms/step\n",
      "Epoch 89/150\n",
      "4/4 - 0s - loss: 0.5148 - accuracy: 0.8100 - 12ms/epoch - 3ms/step\n",
      "Epoch 90/150\n",
      "4/4 - 0s - loss: 0.5111 - accuracy: 0.8100 - 12ms/epoch - 3ms/step\n",
      "Epoch 91/150\n",
      "4/4 - 0s - loss: 0.5076 - accuracy: 0.8100 - 7ms/epoch - 2ms/step\n",
      "Epoch 92/150\n",
      "4/4 - 0s - loss: 0.5042 - accuracy: 0.8000 - 9ms/epoch - 2ms/step\n",
      "Epoch 93/150\n",
      "4/4 - 0s - loss: 0.5010 - accuracy: 0.8000 - 9ms/epoch - 2ms/step\n",
      "Epoch 94/150\n",
      "4/4 - 0s - loss: 0.4979 - accuracy: 0.8000 - 7ms/epoch - 2ms/step\n",
      "Epoch 95/150\n",
      "4/4 - 0s - loss: 0.4948 - accuracy: 0.7900 - 8ms/epoch - 2ms/step\n",
      "Epoch 96/150\n",
      "4/4 - 0s - loss: 0.4923 - accuracy: 0.7900 - 9ms/epoch - 2ms/step\n",
      "Epoch 97/150\n",
      "4/4 - 0s - loss: 0.4892 - accuracy: 0.7800 - 8ms/epoch - 2ms/step\n",
      "Epoch 98/150\n",
      "4/4 - 0s - loss: 0.4858 - accuracy: 0.7900 - 9ms/epoch - 2ms/step\n",
      "Epoch 99/150\n",
      "4/4 - 0s - loss: 0.4827 - accuracy: 0.8200 - 8ms/epoch - 2ms/step\n",
      "Epoch 100/150\n",
      "4/4 - 0s - loss: 0.4799 - accuracy: 0.8200 - 6ms/epoch - 1ms/step\n",
      "Epoch 101/150\n",
      "4/4 - 0s - loss: 0.4769 - accuracy: 0.8500 - 7ms/epoch - 2ms/step\n",
      "Epoch 102/150\n",
      "4/4 - 0s - loss: 0.4743 - accuracy: 0.8700 - 8ms/epoch - 2ms/step\n",
      "Epoch 103/150\n",
      "4/4 - 0s - loss: 0.4716 - accuracy: 0.8600 - 7ms/epoch - 2ms/step\n",
      "Epoch 104/150\n",
      "4/4 - 0s - loss: 0.4689 - accuracy: 0.8500 - 9ms/epoch - 2ms/step\n",
      "Epoch 105/150\n",
      "4/4 - 0s - loss: 0.4667 - accuracy: 0.8300 - 7ms/epoch - 2ms/step\n",
      "Epoch 106/150\n",
      "4/4 - 0s - loss: 0.4651 - accuracy: 0.8100 - 10ms/epoch - 2ms/step\n",
      "Epoch 107/150\n",
      "4/4 - 0s - loss: 0.4638 - accuracy: 0.7900 - 9ms/epoch - 2ms/step\n",
      "Epoch 108/150\n",
      "4/4 - 0s - loss: 0.4618 - accuracy: 0.7900 - 8ms/epoch - 2ms/step\n",
      "Epoch 109/150\n",
      "4/4 - 0s - loss: 0.4595 - accuracy: 0.7900 - 10ms/epoch - 2ms/step\n",
      "Epoch 110/150\n",
      "4/4 - 0s - loss: 0.4569 - accuracy: 0.7900 - 9ms/epoch - 2ms/step\n",
      "Epoch 111/150\n",
      "4/4 - 0s - loss: 0.4538 - accuracy: 0.8000 - 7ms/epoch - 2ms/step\n",
      "Epoch 112/150\n",
      "4/4 - 0s - loss: 0.4508 - accuracy: 0.8200 - 11ms/epoch - 3ms/step\n",
      "Epoch 113/150\n",
      "4/4 - 0s - loss: 0.4477 - accuracy: 0.8700 - 7ms/epoch - 2ms/step\n",
      "Epoch 114/150\n",
      "4/4 - 0s - loss: 0.4445 - accuracy: 0.8700 - 8ms/epoch - 2ms/step\n",
      "Epoch 115/150\n",
      "4/4 - 0s - loss: 0.4417 - accuracy: 0.9000 - 8ms/epoch - 2ms/step\n",
      "Epoch 116/150\n",
      "4/4 - 0s - loss: 0.4405 - accuracy: 0.9200 - 6ms/epoch - 2ms/step\n",
      "Epoch 117/150\n",
      "4/4 - 0s - loss: 0.4394 - accuracy: 0.9200 - 9ms/epoch - 2ms/step\n",
      "Epoch 118/150\n",
      "4/4 - 0s - loss: 0.4364 - accuracy: 0.9200 - 6ms/epoch - 1ms/step\n",
      "Epoch 119/150\n",
      "4/4 - 0s - loss: 0.4340 - accuracy: 0.9200 - 11ms/epoch - 3ms/step\n",
      "Epoch 120/150\n",
      "4/4 - 0s - loss: 0.4312 - accuracy: 0.9200 - 10ms/epoch - 2ms/step\n",
      "Epoch 121/150\n",
      "4/4 - 0s - loss: 0.4287 - accuracy: 0.9200 - 5ms/epoch - 1ms/step\n",
      "Epoch 122/150\n",
      "4/4 - 0s - loss: 0.4257 - accuracy: 0.9200 - 11ms/epoch - 3ms/step\n",
      "Epoch 123/150\n",
      "4/4 - 0s - loss: 0.4240 - accuracy: 0.9000 - 8ms/epoch - 2ms/step\n",
      "Epoch 124/150\n",
      "4/4 - 0s - loss: 0.4222 - accuracy: 0.9000 - 8ms/epoch - 2ms/step\n",
      "Epoch 125/150\n",
      "4/4 - 0s - loss: 0.4202 - accuracy: 0.8800 - 10ms/epoch - 2ms/step\n",
      "Epoch 126/150\n",
      "4/4 - 0s - loss: 0.4176 - accuracy: 0.9000 - 6ms/epoch - 2ms/step\n",
      "Epoch 127/150\n",
      "4/4 - 0s - loss: 0.4149 - accuracy: 0.9200 - 8ms/epoch - 2ms/step\n",
      "Epoch 128/150\n",
      "4/4 - 0s - loss: 0.4123 - accuracy: 0.9200 - 9ms/epoch - 2ms/step\n",
      "Epoch 129/150\n",
      "4/4 - 0s - loss: 0.4102 - accuracy: 0.9200 - 7ms/epoch - 2ms/step\n",
      "Epoch 130/150\n",
      "4/4 - 0s - loss: 0.4081 - accuracy: 0.9200 - 10ms/epoch - 2ms/step\n",
      "Epoch 131/150\n",
      "4/4 - 0s - loss: 0.4064 - accuracy: 0.9100 - 8ms/epoch - 2ms/step\n",
      "Epoch 132/150\n",
      "4/4 - 0s - loss: 0.4048 - accuracy: 0.9000 - 8ms/epoch - 2ms/step\n",
      "Epoch 133/150\n",
      "4/4 - 0s - loss: 0.4024 - accuracy: 0.9200 - 10ms/epoch - 3ms/step\n",
      "Epoch 134/150\n",
      "4/4 - 0s - loss: 0.3999 - accuracy: 0.9200 - 8ms/epoch - 2ms/step\n",
      "Epoch 135/150\n",
      "4/4 - 0s - loss: 0.3975 - accuracy: 0.9200 - 7ms/epoch - 2ms/step\n",
      "Epoch 136/150\n",
      "4/4 - 0s - loss: 0.3955 - accuracy: 0.9200 - 11ms/epoch - 3ms/step\n",
      "Epoch 137/150\n",
      "4/4 - 0s - loss: 0.3938 - accuracy: 0.9100 - 10ms/epoch - 2ms/step\n",
      "Epoch 138/150\n",
      "4/4 - 0s - loss: 0.3918 - accuracy: 0.9300 - 7ms/epoch - 2ms/step\n",
      "Epoch 139/150\n",
      "4/4 - 0s - loss: 0.3907 - accuracy: 0.9300 - 10ms/epoch - 2ms/step\n",
      "Epoch 140/150\n",
      "4/4 - 0s - loss: 0.3884 - accuracy: 0.9300 - 7ms/epoch - 2ms/step\n",
      "Epoch 141/150\n",
      "4/4 - 0s - loss: 0.3848 - accuracy: 0.9300 - 8ms/epoch - 2ms/step\n",
      "Epoch 142/150\n",
      "4/4 - 0s - loss: 0.3834 - accuracy: 0.9200 - 9ms/epoch - 2ms/step\n",
      "Epoch 143/150\n",
      "4/4 - 0s - loss: 0.3810 - accuracy: 0.9200 - 7ms/epoch - 2ms/step\n",
      "Epoch 144/150\n",
      "4/4 - 0s - loss: 0.3800 - accuracy: 0.9200 - 8ms/epoch - 2ms/step\n",
      "Epoch 145/150\n",
      "4/4 - 0s - loss: 0.3791 - accuracy: 0.9200 - 9ms/epoch - 2ms/step\n",
      "Epoch 146/150\n",
      "4/4 - 0s - loss: 0.3771 - accuracy: 0.9100 - 7ms/epoch - 2ms/step\n",
      "Epoch 147/150\n",
      "4/4 - 0s - loss: 0.3759 - accuracy: 0.9100 - 8ms/epoch - 2ms/step\n",
      "Epoch 148/150\n",
      "4/4 - 0s - loss: 0.3737 - accuracy: 0.9200 - 6ms/epoch - 2ms/step\n",
      "Epoch 149/150\n",
      "4/4 - 0s - loss: 0.3707 - accuracy: 0.9200 - 6ms/epoch - 2ms/step\n",
      "Epoch 150/150\n",
      "4/4 - 0s - loss: 0.3683 - accuracy: 0.9200 - 7ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x238a7f94910>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_x_train,y_train,epochs=150,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTING THE ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "[2 1 2 2 2 1 2 2 2 2 2 1 1 1 1 2 2 2 2 2 1 2 1 2 2 2 2 2 1 1 1 1 2 1 1 2 2\n",
      " 1 1 1 2 2 2 1 1 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob=model.predict(scaled_x_test)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous cell returned the probabilites, however we need the one hunted coding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 2 0 1 2 2 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0 1 2 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "# y_pred=model.predict_classes(scaled_x_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0 1 2 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(y_test,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\CODING\\AIML\\Natural Language Processing\\Deep learning NLP\\keras_basics.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CODING/AIML/Natural%20Language%20Processing/Deep%20learning%20NLP/keras_basics.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pred\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39;49mpredict_classes(scaled_x_test)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "pred=model.predict_classes(scaled_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0]\n",
      " [ 0 13  2]\n",
      " [ 0  0 16]]\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "print(confusion_matrix(y_test.argmax(axis=1),y_pred))\n",
    "print(accuracy_score(y_test.argmax(axis=1),y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
